
we have documents - d1,d2,d3, ...dn 
 - aim of lda 
	- what are the most important topic 
	- what are the topics which are assigned to each document.
 
lda ouptut 
===============
	topic1 = 30% fruits, 15% eggs, 10% biscuts .....(food)
	topic2 = 20% lion, 10% dogs , 5% zoo .....(animals)
	.... 

	d1 = 100% topic1 
	d2 = 100% topic1 
	d3 = 49% topic1 and 51% topic2 .. 
	...

In topic modelling , the input is a document-term matrix, where each row represents a document and columns are word count. Each topic will consist of a set of words where order doesn't matter, so we're going to start with the bag of words format. 

gensim: gensim is a python toolkit for topic modelling. we are going to a particualr topic modelling technique called latent dirichlet allocation(LDA). We're also going to use nltk for some parts-of-speech tagging. 


output - our goal is to find themes across various comedy routines, and see which comedians tend to talk about which themes. 

LATENT DIRICHLET ALLOATION 
===============================
latent - hidden : finding hidden topics 
dirichlet - type of probability distribution 

doc1 = {i like bananas and orages} --------------100% topic a 
doc2 = {frogs and fish live in pandas} -------------- 100% topic b 
doc3 = {kittens and puppies and fluffy} ------------- 100% topic b 
doc4 = {i had a spinach and apple smoothie} ------------ 100% topic a 
doc5 = {my kitten loves kale}    ------------- 60% topic a , 40% topic b 


topic a = {40 % banana, 30% kale, 10% breakfast ....}  ===> food 
topic b = { 30% kitten, 20% puppy, 10% frog, 5% cute, ....} ===> animals 



--> Every document consists of a mix of topics ( distribution of topics) 
--> Every topic consists of a mix of words  ( probability distribution of words) 



