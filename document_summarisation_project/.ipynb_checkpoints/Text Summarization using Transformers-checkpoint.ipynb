{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e44df6",
   "metadata": {},
   "source": [
    "## Text Summarization Using Transformers - Literature Review "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ac882",
   "metadata": {},
   "source": [
    "<img src='https://64.media.tumblr.com/187b9c8fdd63482b136548533d6089b4/377214bb249a8e1e-2c/s640x960/39be2d99bb8e53d546a1d9af19893cae7fb7a35c.png'> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82266608",
   "metadata": {},
   "source": [
    "It is the ability to explain a larger piece of literature in short and covering most of the meaning the context address.\n",
    " * Extractive Summarization \n",
    "  Involves extracting particular pieces of text based on prefefined weights assigned to the important words where the selection of the text depends on the weights of the words in it. Default weights are assigned according to the frequency of occurence of a word. \n",
    " * Abstractive Summarization \n",
    "Abstractive summarization includes heuristic approaches to train the system in making an attempt to undersand the whole context and generate a summary based on that understanding. This is a more human like way of generating summaries and these summaries are more effective as compared to extractive approaches. \n",
    "\n",
    "## Sequence to sequence models \n",
    "The sequence to sequence encode-decoder architecture is the base for sequence transfuction tasks. The seq2seq model consists of seperate RNNs at encoder and decoder respectively. Using this encoder sequence and word-level generative modeling, seq2seq generates the target sequence. Since encoding is at the word level, for longer sequences it is difficult to preserve the context at the encoder,  hence the well known attention mechanism was incorporated with seq2seq to pary attention at specific words in the sequence that prominently contribute to the generatio of the target sequence. Attention is weighting individual words in the input sequence according to the impact they make on the target sequence generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7564fe8f",
   "metadata": {},
   "source": [
    "### Paper 1 : Abstractive summarization of long medical documents with trasformers\n",
    " * Long document summarizatio using transformers \n",
    " * Performed the summarization task using two steps, extractive and abstractive step. \n",
    " * used pubmed dataset \n",
    " * Results showed that use of pretrained transformers lead to improvements in both extractive and abstractive steps. \n",
    " * Transformers have limited-size context windows which limit their ability to perform summarization over long documents; the mixed extractive and abstractive approach attempts to remedy this issue. The models used for extractive and abstractive summarization are trained seperately, and then used sequentially to perform our mixed summarization approach. \n",
    " * In the first step, important sentences are extracted from a document through the use of a BERT based extractive summarizer. This summarizer creates sentence-level embeddings for each sentence in the document, which are then passed into either a classifier or cluster algorithm(k means) to identify the best summary sentences. \n",
    " * In the second step extracted sentences are fed as input to the BART transformer model. The transformer then creates a summary based off these extracted sentences. Performing the extractive step allows the summary to be conditioned on important sentecnes from throughout the document, despite the limitees context window.\n",
    " * Evaluation method - used ROUGE scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10aa4b6",
   "metadata": {},
   "source": [
    "### Paper 2: Evaluating Extractive Text Summarization with BERTSUM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe54445",
   "metadata": {},
   "source": [
    "* Main objective : evaluating BERTSUM using ROUGE metrices. \n",
    "* Used - prepared CNN/DailyMail dataset. \n",
    "* Note: The goal of extractive text summarization models is to score each sentence in the document to be able to include the most relevant sentences in the summary. In the case of abstractive summarization there is a need for the model to have word generative capabilities given words or context that might not be included in the document. The progress in the extractive text summarization has seen remarkable accuracy thanks to models like BERTSUM which uses fine tuning layers to add document based context from the BERT outputs to more efficient models such as DistllBert which shows relatively similar performance but needs a lot less space and time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2398b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
