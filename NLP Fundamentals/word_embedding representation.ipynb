{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116dc590",
   "metadata": {},
   "source": [
    "# What are word embeddings \n",
    "A word embedding is a learned representation for text where words that have the same meaning have a similar representation. It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems\n",
    "\n",
    "# Word2vec Representation\n",
    "Word2vec is a recently-introduced distributed word representation learning \n",
    "technique that is currently being used as a feature engineering technique for many \n",
    "NLP tasks\n",
    "\n",
    "### Advantages \n",
    " * The Word2vec approach is not subjective to the human knowledge of language as in the WordNet-based approach.\n",
    " * Word2vec representation vector size is independent of the vocabulary size unlike one-hot encoded representation or the word co-occurrence matrix\n",
    " * Word2vec is a distributed representation. Unlike localist representation, where the representation depends on the activation of a single element of the representation vector (for example, one-hot encoding), the distributed representation depends on the activation pattern of all the elements in the vector. This gives more expressive power to Word2vec than produced by the one-hot encoded representation\n",
    " \n",
    "### Main idea \n",
    "Word2vec learns the meaning of a given word by looking at its context and representing it numerically. By context, we refer to a fixed number of words in front of and behind the word of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab5791",
   "metadata": {},
   "source": [
    "# The continous bad of words model \n",
    "The CBOW model architecture tries to predict the current target word (the center word) based on the source context words (surrounding words).\n",
    "\n",
    "Considering a simple sentence, “the quick brown fox jumps over the lazy dog”, this can be pairs of (context_window, target_word) where if we consider a context window of size 2, we have examples like ([quick, fox], brown), ([the, brown], quick), ([the, dog], lazy) and so on. Thus the model tries to predict the target_word based on the context_window words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac255e6",
   "metadata": {},
   "source": [
    "<img src = 'https://www.researchgate.net/profile/Daniel-Braun-6/publication/326588219/figure/fig1/AS:652185784295425@1532504616288/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram-SG-training-model-illustrations.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e209e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "import tensorflow\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58946c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "776e57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =[\n",
    "    'the glass of milk',\n",
    "    'the cup of tea',\n",
    "    'I am a good boy',\n",
    "    'I am a good developer',\n",
    "    'Understand the meaning of words'   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c62e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size  =1000\n",
    "labels = np.array([1,1,0,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75087478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[660, 768, 298, 252],\n",
       " [660, 637, 298, 953],\n",
       " [198, 910, 854, 56, 955],\n",
       " [198, 910, 854, 56, 679],\n",
       " [87, 660, 321, 298, 133]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = [one_hot(words,voc_size) for words in corpus]\n",
    "onehot  # getting the index representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac803df",
   "metadata": {},
   "source": [
    "# Word embedding representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7a26c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5482022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba98cbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0 660 768 298 252]\n",
      " [  0   0   0   0 660 637 298 953]\n",
      " [  0   0   0 198 910 854  56 955]\n",
      " [  0   0   0 198 910 854  56 679]\n",
      " [  0   0   0  87 660 321 298 133]]\n"
     ]
    }
   ],
   "source": [
    "sent_length = 8 \n",
    "embedded_docs = pad_sequences(onehot, padding = 'pre', maxlen = sent_length) \n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b747f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 10\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09e51bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, dim, input_length = sent_length))\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(2,activation = 'softmax')) \n",
    "model.compile('adam','categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d480af83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 8, 10)             10000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 162       \n",
      "=================================================================\n",
      "Total params: 10,162\n",
      "Trainable params: 10,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22c2101e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0, 660, 768, 298, 252])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e77a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f5d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
