{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847ab7cc",
   "metadata": {},
   "source": [
    "### Dataset - sarcasm detection in news headlines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7bbfb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 20:33:30.908908: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-22 20:33:30.908986: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['README.md',\n",
       " 'NLP LABS',\n",
       " 'Document Summarisation Project',\n",
       " '.git',\n",
       " 'NLP Fundamentals',\n",
       " 'spacy_cheatsheets',\n",
       " 'Topic Modelling',\n",
       " 'data',\n",
       " 'pretrained_word_embeddings.ipynb',\n",
       " 'Stock price Prediction Using NewsHeadlines',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    " \n",
    "# get current directory\n",
    "cwd = os.getcwd()\n",
    " \n",
    "# prints parent directory\n",
    "parent_directory = os.path.abspath(os.path.join(cwd, os.pardir))\n",
    "os.listdir(parent_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e676ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'{parent_directory}/data/Sarcasm_Headlines_Dataset.json'\n",
    "import json \n",
    "with open(filename,'r') as f:\n",
    "    data = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701a5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "d = data.split('\\n')\n",
    "for i in range(len(d)):\n",
    "    try:\n",
    "        document = json.loads(d[i])\n",
    "        url  = document['article_link']\n",
    "        urls.append(url)\n",
    "        headline = document['headline']\n",
    "        sentences.append(headline)\n",
    "        label = document['is_sarcastic']\n",
    "        labels.append(label)\n",
    "    except:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008fb102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
       " \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
       " \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
       " 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas',\n",
       " 'j.k. rowling wishes snape happy birthday in the most magical way',\n",
       " \"advancing the world's women\",\n",
       " 'the fascinating case for eating lab-grown meat',\n",
       " 'this ceo will send your kids to school, if you work for his company',\n",
       " 'top snake handler leaves sinking huckabee campaign',\n",
       " \"friday's morning email: inside trump's presser for the ages\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cbfb768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"former versace store clerk sues over secret 'black code' for minority shoppers\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 20000\n",
    "\n",
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]\n",
    "lentraining_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cba65d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f87a39de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[328, 1, 799, 3405, 2404, 47, 389, 2214, 1, 6, 2614, 8863]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62a83b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this block to get it to work with TensorFlow 2.x\n",
    "import numpy as np\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d819a02",
   "metadata": {},
   "source": [
    "## Embedding for getting the meaning of a word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ac31d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60a5fb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 160,433\n",
      "Trainable params: 160,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3158985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9963 - val_loss: 1.5439 - val_accuracy: 0.8006\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 1.5972 - val_accuracy: 0.8047\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9967 - val_loss: 1.7250 - val_accuracy: 0.8056\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 1.7789 - val_accuracy: 0.8047\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0093 - accuracy: 0.9973 - val_loss: 1.7810 - val_accuracy: 0.8044\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0084 - accuracy: 0.9977 - val_loss: 1.7564 - val_accuracy: 0.8046\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0074 - accuracy: 0.9984 - val_loss: 1.8197 - val_accuracy: 0.8044\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0085 - accuracy: 0.9977 - val_loss: 1.9128 - val_accuracy: 0.8061\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0069 - accuracy: 0.9984 - val_loss: 1.9456 - val_accuracy: 0.8055\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0086 - accuracy: 0.9973 - val_loss: 1.9738 - val_accuracy: 0.8055\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(\n",
    "    training_padded, \n",
    "    training_labels, \n",
    "    epochs=num_epochs, \n",
    "    validation_data=(testing_padded, testing_labels),\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862d19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21cfa255",
   "metadata": {},
   "source": [
    "### Testing examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcca3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = [\n",
    "    'granny starting to fear  spiders in the garden might be real',\n",
    "    'the weather today is bright and sunny'\n",
    "]\n",
    "new_sequences = tokenizer.texts_to_sequences(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddc101d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(\n",
    "    new_sequences, \n",
    "    maxlen = max_length, \n",
    "    padding = padding_type,\n",
    "    truncating = trunc_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9fd61d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.876030\n",
      "0.000000\n"
     ]
    }
   ],
   "source": [
    "for i in model.predict(padded):\n",
    "    print(format(i[0],'f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89c41136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.876030\n"
     ]
    }
   ],
   "source": [
    "print(format(8.7602985e-01, 'f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90910140",
   "metadata": {},
   "source": [
    "## LSTM MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a721d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000,64), \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)), \n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1,activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0eed54c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 64)          640000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 714,369\n",
      "Trainable params: 714,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b30c97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 37s 55ms/step - loss: 0.3872 - accuracy: 0.8148 - val_loss: 0.3226 - val_accuracy: 0.8618\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.1913 - accuracy: 0.9240 - val_loss: 0.3482 - val_accuracy: 0.8594\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 41s 65ms/step - loss: 0.1057 - accuracy: 0.9618 - val_loss: 0.3985 - val_accuracy: 0.8457\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 40s 65ms/step - loss: 0.0631 - accuracy: 0.9779 - val_loss: 0.5032 - val_accuracy: 0.8469\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 38s 61ms/step - loss: 0.0355 - accuracy: 0.9880 - val_loss: 0.6453 - val_accuracy: 0.8389\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.0212 - accuracy: 0.9930 - val_loss: 0.7462 - val_accuracy: 0.8430\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 38s 60ms/step - loss: 0.0127 - accuracy: 0.9962 - val_loss: 0.8206 - val_accuracy: 0.8405\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 41s 65ms/step - loss: 0.0093 - accuracy: 0.9967 - val_loss: 0.9356 - val_accuracy: 0.8407\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 41s 66ms/step - loss: 0.0122 - accuracy: 0.9961 - val_loss: 0.9804 - val_accuracy: 0.8348\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 41s 66ms/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 1.0272 - val_accuracy: 0.8337\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = lstm_model.fit(\n",
    "    training_padded, \n",
    "    training_labels, \n",
    "    epochs=num_epochs, \n",
    "    validation_data=(testing_padded, testing_labels),\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a9e3d2",
   "metadata": {},
   "source": [
    "### Notes\n",
    "In practical terms, you can use the pretrained Word2vec embeddings as features of any neural network (or other algorithm). They can give you advantage if your data is small, since the pretrained embeddings were trained on large volumes of text.\n",
    "\n",
    "On another hand, there are examples showing that learning the embeddings from your data, optimized for a particular problem, may be more efficient "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced5474",
   "metadata": {},
   "source": [
    "\n",
    "Embeddings are methods for learning vector representations of categorical data. They are most commonly used for working with textual data. Word2vec and GloVe are two popular frameworks for learning word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3639a60",
   "metadata": {},
   "source": [
    "For Keras Embedding Layer, You are using supervised learning. Embedding learned here for independent variable will directly map to the dependent variable.\n",
    "However, word2vec or glove is unsupervised learning problem. Here, embedding learned depends on data you are feeding to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada36349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
